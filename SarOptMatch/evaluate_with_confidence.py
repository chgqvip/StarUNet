# SarOptMatch/evaluate_with_confidence.py
import traceback

import tensorflow as tf
import numpy as np
from tqdm import tqdm
import sys
import matplotlib.pyplot as plt
import os
import cv2  # <--- ç¡®ä¿å¯¼å…¥ OpenCV

try:
    # å°è¯•ç›¸å¯¹è·¯å¾„å¯¼å…¥
    from SarOptMatch.evaluation import print_perf_table
except ImportError as e_main:
    print(f"è­¦å‘Š: æ— æ³•é€šè¿‡ç›¸å¯¹è·¯å¾„å¯¼å…¥ SarOptMatch.evaluation: {e_main}ã€‚å°è¯•ç›´æ¥å¯¼å…¥ã€‚")
    try:
        # å¦‚æœæ¨¡å—ç»“æ„æ˜¯ SarOptMatch.evaluationï¼Œå°è¯•ç›´æ¥å¯¼å…¥
        import SarOptMatch.evaluation as evaluation_module

        print_perf_table = evaluation_module.print_perf_table
        print("æˆåŠŸé€šè¿‡ç›´æ¥å¯¼å…¥ SarOptMatch.evaluation å¯¼å…¥ print_perf_tableã€‚")
    except ImportError as e_direct:
        print(f"é”™è¯¯: ä» SarOptMatch.evaluation å¯¼å…¥ print_perf_table å¤±è´¥: {e_direct}ã€‚")


        # å ä½ç¬¦å‡½æ•°ï¼Œç¡®ä¿å®ƒä¸ä½¿ç”¨ group_name
        def print_perf_table(euc_dists, dx_errors=None, dy_errors=None):  # ç§»é™¤äº† **kwargs ä»¥åŒ¹é…é”™è¯¯
            print("--- å ä½ç¬¦ print_perf_table ---")
            if euc_dists is not None and euc_dists.size > 0:
                print(f"  å¹³å‡æ¬§æ°è·ç¦»: {np.mean(euc_dists):.4f}")
            else:
                print("  æ¬§æ°è·ç¦»: æ— ")

            if dx_errors is not None and dx_errors.size > 0:
                print(f"  å¹³å‡Xæ–¹å‘åç§» (é¢„æµ‹ - çœŸå®): {np.mean(dx_errors):.4f} åƒç´ ")
                print(f"  å¹³å‡Xæ–¹å‘ç»å¯¹åç§»: {np.mean(np.abs(dx_errors)):.4f} åƒç´ ")
            else:
                print("  dx_errors: æ— ")

            if dy_errors is not None and dy_errors.size > 0:
                print(f"  å¹³å‡Yæ–¹å‘åç§» (é¢„æµ‹ - çœŸå®): {np.mean(dy_errors):.4f} åƒç´ ")
                print(f"  å¹³å‡Yæ–¹å‘ç»å¯¹åç§»: {np.mean(np.abs(dy_errors)):.4f} åƒç´ ")
            else:
                print("  dy_errors: æ— ")
            print("--- å ä½ç¬¦ç»“æŸ ---")


def calculate_entropy(heatmap_probs):
    """è®¡ç®—å•ä¸ªçƒ­å›¾çš„ç†µå€¼ (åŸºäºæ¦‚ç‡åˆ†å¸ƒ)"""
    heatmap_probs = np.maximum(heatmap_probs, 1e-12)  # é¿å… log(0) é”™è¯¯
    entropy = -np.sum(heatmap_probs * np.log2(heatmap_probs))
    return entropy


def spatial_softmax_numpy(raw_heatmap):
    """å¯¹å•ä¸ªåŸå§‹çƒ­å›¾åº”ç”¨ç©ºé—´Softmax (NumPyç‰ˆæœ¬)ï¼Œè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ"""
    flat_heatmap = raw_heatmap.flatten()
    # æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
    exp_heatmap = np.exp(flat_heatmap - np.max(flat_heatmap))
    softmax_flat = exp_heatmap / np.sum(exp_heatmap)
    return softmax_flat.reshape(raw_heatmap.shape)


def run_confidence_evaluation_with_entropy(
        model_to_evaluate: tf.keras.Model,
        validation_data_for_confidence: tf.data.Dataset,
        model_name_for_log: str,
        entropy_percentile_for_selection=20,  # ç”¨äºåˆ’åˆ†ä½ç†µ/é«˜ç†µç»„çš„ç™¾åˆ†ä½æ•°
        log_highest_entropy_samples_percentile=80,  # ç”¨äºåœ¨ç›´æ–¹å›¾ä¸Šæ ‡è®°çš„ç†µç™¾åˆ†ä½æ•°
        # === RANSAC ç›¸å…³å‚æ•° (ä»ä¸»è°ƒç”¨è„šæœ¬ä¼ å…¥) ===
        apply_ransac_on_low_entropy=False,  # æ˜¯å¦åœ¨ä½ç†µç»„ä¸Šåº”ç”¨RANSAC
        ransac_reproj_threshold=5.0,  # RANSAC é‡æŠ•å½±é˜ˆå€¼ (åƒç´ )
        min_ransac_samples=4,  # RANSAC æ‰€éœ€æœ€å°æ ·æœ¬æ•°
        ##### æ–°å¢/ä¿®æ”¹ å¼€å§‹ #####
        run_ransac_only_on_all_samples_mode=False,  # æ–°å¢å‚æ•°ï¼Œæ§åˆ¶æ˜¯å¦åªè¿è¡ŒRANSACæ¨¡å¼
        ##### æ–°å¢/ä¿®æ”¹ ç»“æŸ #####
        output_dir="evaluation_results"  # è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
):
    M_affine_all = None  # ä¸º RANSAC-only æ¨¡å¼ä¸‹çš„å…¨å±€ä»¿å°„çŸ©é˜µåˆå§‹åŒ–
    M_affine = None  # ä¸ºç†µæ¨¡å¼ä¸‹ä½ç†µç»„çš„ä»¿å°„çŸ©é˜µåˆå§‹åŒ–

    if run_ransac_only_on_all_samples_mode:
        print(f"\n--- ğŸ›¡ï¸ å¼€å§‹å¯¹æ¨¡å‹ '{model_name_for_log}' è¿›è¡Œä»…RANSACè¯„ä¼° (åœ¨æ‰€æœ‰æ ·æœ¬ä¸Š) ---")
        print(f"RANSACå‚æ•°: é‡æŠ•å½±é˜ˆå€¼: {ransac_reproj_threshold}åƒç´ , æœ€å°‘æ ·æœ¬æ•°: {min_ransac_samples}")
    else:
        print(f"\n--- ğŸ›¡ï¸ å¼€å§‹å¯¹æ¨¡å‹ '{model_name_for_log}' è¿›è¡ŒåŸºäºç†µå€¼çš„è¯„ä¼° ---")
        print(f"ä½ç†µç»„å®šä¹‰: ç†µå€¼æœ€ä½çš„ {entropy_percentile_for_selection}% çš„æ ·æœ¬")
        if apply_ransac_on_low_entropy:  # æ­¤å‚æ•°ä»…åœ¨ç†µæ¨¡å¼ä¸‹æœ‰æ„ä¹‰
            print(
                f"RANSACè¯„ä¼° (ç†µæ¨¡å¼): å°†åœ¨ä½ç†µç»„ä¸Šåº”ç”¨RANSAC (é‡æŠ•å½±é˜ˆå€¼: {ransac_reproj_threshold}åƒç´ , æœ€å°‘æ ·æœ¬æ•°: {min_ransac_samples})")


    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºä¸€ä¸ªå­ç›®å½•ï¼Œæ›¿æ¢æ‰æ–‡ä»¶åä¸­ä¸åˆæ³•å­—ç¬¦
    model_output_dir = os.path.join(output_dir,
                                    model_name_for_log.replace(' ', '_').replace('/', '_').replace(':', '_'))
    os.makedirs(model_output_dir, exist_ok=True)

    # =================================================================
    # vvvvvvvvvvvvvvvv ã€æ ¸å¿ƒä¿®æ”¹ï¼šåˆ†æ­¥å¤„ç†ä»¥é¿å…OOMã€‘ vvvvvvvvvvvvvvvv
    # =================================================================

    # --- æ­¥éª¤ 1: åˆ†æ‰¹é¢„æµ‹ä¸æ•°æ®æ”¶é›† ---
    all_raw_heatmaps_list = []
    all_gt_offsets_list = []

    desc_str = f"æ¨¡å‹é¢„æµ‹ ({model_name_for_log})"
    progress_bar = tqdm(validation_data_for_confidence, desc=desc_str, leave=False, file=sys.stdout, unit="batch")

    for batch_idx, batch_data in enumerate(progress_bar):
        try:
            # å…¼å®¹ä¸åŒæ•°æ®é›†ç»“æ„
            if len(batch_data) == 2 and isinstance(batch_data[0], tuple) and isinstance(batch_data[1], tuple):
                (opt_im_batch, sar_im_batch), (_, gt_original_offsets_batch) = batch_data
            else:
                opt_im_batch, sar_im_batch, _, gt_original_offsets_batch = batch_data

            raw_heatmaps_batch = model_to_evaluate.predict_on_batch([opt_im_batch, sar_im_batch])

            all_raw_heatmaps_list.append(raw_heatmaps_batch)
            all_gt_offsets_list.append(gt_original_offsets_batch)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: åœ¨æ‰¹æ¬¡ {batch_idx} ä¸Šè¿›è¡Œæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            print(f"   å…‰å­¦å›¾åƒæ‰¹æ¬¡å½¢çŠ¶: {opt_im_batch.shape}, SARå›¾åƒæ‰¹æ¬¡å½¢çŠ¶: {sar_im_batch.shape}")
            traceback.print_exc()
            continue

    if not all_raw_heatmaps_list:
        print("æœªå¤„ç†ä»»ä½•æœ‰æ•ˆæ ·æœ¬ã€‚è¯„ä¼°ä¸­æ­¢ã€‚")
        return [], None

    # --- æ­¥éª¤ 2: æ±‡æ€»æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ ---
    print(f"\n...æ‰€æœ‰æ‰¹æ¬¡é¢„æµ‹å®Œæˆï¼Œæ­£åœ¨æ±‡æ€» {len(all_raw_heatmaps_list)} ä¸ªæ‰¹æ¬¡çš„ç»“æœ...")
    # ä½¿ç”¨ .numpy() ç¡®ä¿ä»Tensorè½¬æ¢ä¸ºNumpyæ•°ç»„åå†æ‹¼æ¥
    full_raw_heatmaps = np.concatenate([h for h in all_raw_heatmaps_list], axis=0)
    full_gt_offsets = np.concatenate([o.numpy() if hasattr(o, 'numpy') else o for o in all_gt_offsets_list], axis=0)
    print(f"   æˆåŠŸæ±‡æ€»äº† {full_raw_heatmaps.shape[0]} ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœã€‚")

    # --- æ­¥éª¤ 3: ä½¿ç”¨æ±‡æ€»åçš„æ•°æ®è¿›è¡Œåç»­æ‰€æœ‰åˆ†æ ---
    sample_metrics_data = []
    for i in range(full_raw_heatmaps.shape[0]):
        raw_heatmap_sample = full_raw_heatmaps[i, :, :, 0]
        gt_offset_numpy = full_gt_offsets[i]

        gt_offset_y, gt_offset_x = gt_offset_numpy
        pred_offset_y_final, pred_offset_x_final = np.unravel_index(np.argmax(raw_heatmap_sample),
                                                                    raw_heatmap_sample.shape)

        gt_np_coords = np.array([gt_offset_y, gt_offset_x])
        pred_np_coords = np.array([pred_offset_y_final, pred_offset_x_final])

        euc_dist = np.linalg.norm(gt_np_coords - pred_np_coords)
        dy_error = pred_offset_y_final - gt_offset_y
        dx_error = pred_offset_x_final - gt_offset_x

        current_sample_dict = {
            'gt_offset': (gt_offset_y, gt_offset_x),
            'pred_offset': (pred_offset_y_final, pred_offset_x_final),
            'euc_dist': euc_dist,
            'dx_error': dx_error,
            'dy_error': dy_error,
            'index': i
        }

        if not run_ransac_only_on_all_samples_mode:
            prob_heatmap_sample = spatial_softmax_numpy(raw_heatmap_sample)
            entropy_value = calculate_entropy(prob_heatmap_sample)
            current_sample_dict['entropy'] = entropy_value

        sample_metrics_data.append(current_sample_dict)

    # =================================================================
    # ^^^^^^^^^^^^^^^^ ã€æ ¸å¿ƒä¿®æ”¹ç»“æŸã€‘ ^^^^^^^^^^^^^^^^


    if not sample_metrics_data:
        print("æœªå¤„ç†ä»»ä½•æœ‰æ•ˆæ ·æœ¬ã€‚è¯„ä¼°ä¸­æ­¢ã€‚")
        return [], None  # è¿”å›ç©ºåˆ—è¡¨å’ŒNone

    num_total_samples = len(sample_metrics_data)  # æ€»æ ·æœ¬æ•°å¯¹ä¸¤ç§æ¨¡å¼éƒ½æœ‰ç”¨
    print(f"\næ€»æœ‰æ•ˆæ ·æœ¬æ•°: {num_total_samples}")  # æå‰æ‰“å°æ€»æ ·æœ¬æ•°

    ##### æ–°å¢/ä¿®æ”¹ å¼€å§‹ #####
    if run_ransac_only_on_all_samples_mode:
        # --- ä»… RANSAC è¯„ä¼° (åœ¨æ‰€æœ‰æ ·æœ¬ä¸Š) ---
        if num_total_samples >= min_ransac_samples:
            print(f"\n  --- RANSAC è¯„ä¼° (åœ¨å…¨éƒ¨ {num_total_samples} ä¸ªæ ·æœ¬ä¸Š) ---")
            # OpenCV RANSAC éœ€è¦ç‚¹åæ ‡æ ¼å¼ä¸º (x, y)
            # src_pts: æ¨¡å‹çš„é¢„æµ‹åç§» (pred_offset)ï¼Œä½œä¸ºæºç‚¹
            # dst_pts: çœŸå®çš„åç§» (gt_offset)ï¼Œä½œä¸ºç›®æ ‡ç‚¹
            # RANSACçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå˜æ¢ Mï¼Œä½¿å¾— M(src_pts) â‰ˆ dst_pts
            src_pts_list_all = [[item['pred_offset'][1], item['pred_offset'][0]] for item in
                                sample_metrics_data]  # (x_pred, y_pred)
            dst_pts_list_all = [[item['gt_offset'][1], item['gt_offset'][0]] for item in
                                sample_metrics_data]  # (x_gt, y_gt)

            src_pts_np_all = np.float32(src_pts_list_all).reshape(-1, 1, 2)
            dst_pts_np_all = np.float32(dst_pts_list_all).reshape(-1, 1, 2)

            # M_affine_all åœ¨å‡½æ•°å¼€å§‹æ—¶å·²åˆå§‹åŒ–ä¸º None
            try:
                # ä½¿ç”¨OpenCVçš„RANSACä¼°è®¡ä»¿å°„å˜æ¢çŸ©é˜µ
                # M_affine_all å°† src_pts_np_all (é¢„æµ‹åç§») å˜æ¢åˆ°æ¥è¿‘ dst_pts_np_all (çœŸå®åç§»)
                M_affine_all, inliers_mask_affine_all = cv2.estimateAffine2D(
                    src_pts_np_all, dst_pts_np_all,  # æºç‚¹: é¢„æµ‹åç§», ç›®æ ‡ç‚¹: çœŸå®åç§»
                    method=cv2.RANSAC,
                    ransacReprojThreshold=ransac_reproj_threshold,  # é‡æŠ•å½±é˜ˆå€¼
                    maxIters=2000, confidence=0.99  # æœ€å¤§è¿­ä»£æ¬¡æ•°å’Œç½®ä¿¡åº¦
                )
                if M_affine_all is not None:
                    num_inliers_all = np.sum(inliers_mask_affine_all)  # å†…ç‚¹æ•°é‡
                    inlier_ratio_all = num_inliers_all / num_total_samples if num_total_samples > 0 else 0  # å†…ç‚¹æ¯”ä¾‹
                    print(f"      RANSAC (ä»¿å°„å˜æ¢) ç»“æœ:")
                    print(f"        æ€»ç‚¹æ•° (å…¨éƒ¨æ ·æœ¬): {num_total_samples}")
                    print(f"        å†…ç‚¹æ•°: {num_inliers_all}")
                    print(f"        å†…ç‚¹æ¯”ä¾‹: {inlier_ratio_all:.4f}")
                    print(f"        ä¼°è®¡çš„ä»¿å°„å˜æ¢çŸ©é˜µ M (å°†é¢„æµ‹åç§»å˜æ¢è‡³æ¥è¿‘çœŸå®åç§»):\n{M_affine_all}")

                    if num_inliers_all > 0:
                        # æå–å†…ç‚¹
                        src_inliers_all = src_pts_np_all[inliers_mask_affine_all.ravel() == 1]
                        dst_inliers_all = dst_pts_np_all[inliers_mask_affine_all.ravel() == 1]
                        # å¯¹å†…ç‚¹æºç‚¹åº”ç”¨ä¼°è®¡çš„å˜æ¢
                        transformed_src_inliers_raw_all = cv2.transform(src_inliers_all, M_affine_all)
                        transformed_src_inliers_flat_all = transformed_src_inliers_raw_all.reshape(-1, 2)
                        dst_inliers_flat_all = dst_inliers_all.reshape(-1, 2)
                        # è®¡ç®—å†…ç‚¹çš„é‡æŠ•å½±è¯¯å·®
                        reprojection_errors_inliers_all = np.linalg.norm(
                            transformed_src_inliers_flat_all - dst_inliers_flat_all, axis=1)
                        print(f"        å†…ç‚¹å¹³å‡é‡æŠ•å½±è¯¯å·®: {np.mean(reprojection_errors_inliers_all):.4f} åƒç´ ")
                        print(f"        å†…ç‚¹ä¸­ä½é‡æŠ•å½±è¯¯å·®: {np.median(reprojection_errors_inliers_all):.4f} åƒç´ ")
                        # è®¡ç®—å†…ç‚¹çš„RMSE
                        rmse_inliers = np.sqrt(np.mean(np.square(reprojection_errors_inliers_all)))
                        print(f"        å†…ç‚¹RMSE (M(é¢„æµ‹åç§») vs çœŸå®åç§»): {rmse_inliers:.4f} åƒç´ ")

                    ##### æœ€å°ä¾µå…¥å¼RMSEè®¡ç®— (é’ˆå¯¹æ‰€æœ‰ç‚¹åº”ç”¨å…¨å±€å˜æ¢) #####
                    print(f"\n      --- å…¨å±€RMSEè®¡ç®— (æ‰€æœ‰æ ·æœ¬åº”ç”¨M_affine_all) ---")
                    try:
                        # ä½¿ç”¨ä¼°è®¡çš„ä»¿å°„æ¨¡å‹ M_affine_all å˜æ¢æ‰€æœ‰çš„æºç‚¹ (å³æ‰€æœ‰é¢„æµ‹çš„å±€éƒ¨åç§»)
                        transformed_all_pred_offsets_raw = cv2.transform(src_pts_np_all, M_affine_all)
                        transformed_all_pred_offsets_flat = transformed_all_pred_offsets_raw.reshape(-1, 2)  # (N, 2)

                        # çœŸå®çš„å±€éƒ¨åç§» (ç›®æ ‡ç‚¹)ï¼Œæ‰å¹³åŒ–
                        all_gt_offsets_flat = dst_pts_np_all.reshape(-1, 2)  # (N, 2)

                        # è®¡ç®—åº”ç”¨RANSACæ¨¡å‹åï¼Œæ‰€æœ‰å˜æ¢åçš„é¢„æµ‹åç§»ä¸å®ƒä»¬å¯¹åº”çœŸå®åç§»ä¹‹é—´çš„æ¬§æ°è·ç¦»è¯¯å·®
                        # errors_all_points_after_affine æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªç‚¹å¯¹è¯¯å·®çš„æ•°ç»„
                        errors_all_points_after_affine = np.linalg.norm(
                            transformed_all_pred_offsets_flat - all_gt_offsets_flat, axis=1)

                        # RMSE = sqrt(mean(squared_errors))
                        rmse_all_points_after_affine_correction = np.sqrt(
                            np.mean(np.square(errors_all_points_after_affine)))
                        print(
                            f"        å…¨å±€RMSE (æ‰€æœ‰ {num_total_samples} ä¸ªæ ·æœ¬, M_affine_all(é¢„æµ‹åç§») vs çœŸå®åç§»): {rmse_all_points_after_affine_correction:.4f} åƒç´ ")
                    except Exception as e_rmse_calc:
                        print(f"        é”™è¯¯: è®¡ç®—å…¨å±€RMSEå¤±è´¥: {e_rmse_calc}")
                    ##### æœ€å°ä¾µå…¥å¼RMSEè®¡ç®—ç»“æŸ #####

                else:
                    print(f"      RANSAC (ä»¿å°„å˜æ¢) æœªèƒ½ä¼°è®¡å˜æ¢æ¨¡å‹ (åœ¨å…¨éƒ¨æ ·æœ¬ä¸Š)ã€‚M_affine_all ä¸º Noneã€‚")
            except cv2.error as e_cv2_all:  # OpenCVç‰¹å®šé”™è¯¯
                print(f"      RANSAC (ä»¿å°„å˜æ¢) æ‰§è¡Œå¤±è´¥ (å…¨éƒ¨æ ·æœ¬, cv2.error): {e_cv2_all}")
                if M_affine_all is not None:  # å¦‚æœåœ¨transformä¸­å‡ºé”™ï¼ŒMå¯èƒ½å·²èµ‹å€¼
                    print(f"        é”™è¯¯å‘ç”Ÿæ—¶ M_affine_all çš„å€¼: {M_affine_all}")
            except Exception as e_ransac_all:  # å…¶ä»–é€šç”¨é”™è¯¯
                print(f"      RANSAC (ä»¿å°„å˜æ¢) è¯„ä¼°ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯ (å…¨éƒ¨æ ·æœ¬): {e_ransac_all}")
        else:
            print(f"\n  --- RANSAC è¯„ä¼° (åœ¨å…¨éƒ¨æ ·æœ¬ä¸Š) ---")
            print(f"      æ ·æœ¬æ•° ({num_total_samples}) å°‘äº RANSAC æ‰€éœ€æœ€å°æ ·æœ¬æ•° ({min_ransac_samples})ï¼Œè·³è¿‡RANSACã€‚")

        # å¯é€‰ï¼šæ‰“å°æ‰€æœ‰æ ·æœ¬çš„åŸå§‹è¯¯å·®ç»Ÿè®¡ (æœªç»RANSACæ ¡æ­£çš„)
        print("\n  --- æ‰€æœ‰æ ·æœ¬çš„åŸå§‹è¯¯å·®ç»Ÿè®¡ (ä»…RANSACæ¨¡å¼, æœªç»RANSACæ ¡æ­£) ---")
        all_euc_dists = np.array([item['euc_dist'] for item in sample_metrics_data])
        all_dx_errors = np.array([item['dx_error'] for item in sample_metrics_data])
        all_dy_errors = np.array([item['dy_error'] for item in sample_metrics_data])
        if all_euc_dists.size > 0:
            print_perf_table(all_euc_dists, all_dx_errors, all_dy_errors)
        else:
            print("    æ²¡æœ‰æ ·æœ¬æ•°æ®å¯ä¾›ç»Ÿè®¡ã€‚")

    else:  # è¿™æ˜¯åŸå§‹çš„åŸºäºç†µçš„è¯„ä¼°é€»è¾‘
        ##### æ–°å¢/ä¿®æ”¹ ç»“æŸ #####
        # ç¡®ä¿ sample_metrics_data ä¸­çš„æ¯ä¸ª item éƒ½æœ‰ 'entropy'é”® (åœ¨å‰é¢å·²å¤„ç†)
        sorted_samples_by_entropy = sorted(sample_metrics_data, key=lambda x: x['entropy'])
        # num_total_samples å·²ç»åœ¨å‰é¢ä» sample_metrics_data è®¡ç®—å¾—å‡º

        # ç¡®å®šä½ç†µç»„çš„æ ·æœ¬æ•°é‡
        if entropy_percentile_for_selection > 0:
            num_low_entropy_samples = int(np.ceil(num_total_samples * (entropy_percentile_for_selection / 100.0)))
            if num_low_entropy_samples == 0 and num_total_samples > 0:  # è‡³å°‘é€‰ä¸€ä¸ªï¼Œå¦‚æœç™¾åˆ†æ¯”å¤ªå°å¯¼è‡´ä¸º0ä½†æœ‰æ ·æœ¬
                num_low_entropy_samples = 1
        else:  # å¦‚æœç™¾åˆ†æ¯”ä¸º0ï¼Œåˆ™ä½ç†µç»„ä¸ºç©º
            num_low_entropy_samples = 0
        num_low_entropy_samples = min(num_low_entropy_samples, num_total_samples)  # ä¸èƒ½è¶…è¿‡æ€»æ ·æœ¬æ•°

        low_entropy_group = sorted_samples_by_entropy[:num_low_entropy_samples]
        high_entropy_group = sorted_samples_by_entropy[num_low_entropy_samples:]

        print(f"\n--- æ¨¡å‹ '{model_name_for_log}' åˆ†ç»„è¯„ä¼°ç»“æœ (ç†µæ¨¡å¼) ---")
        # print(f"æ€»æœ‰æ•ˆæ ·æœ¬æ•°: {num_total_samples}") # å·²æå‰æ‰“å°

        # --- ä½ç†µç»„è¯„ä¼° ---
        print(
            f"\n  --- ä½ç†µç»„ (ç†µå€¼æœ€ä½çš„ {entropy_percentile_for_selection}%, å…± {len(low_entropy_group)} ä¸ªæ ·æœ¬) ---")
        low_entropy_threshold_display = -1.0  # ç”¨äºæ˜¾ç¤ºçš„ç†µé˜ˆå€¼
        if low_entropy_group:
            low_entropy_euc_dists = np.array([item['euc_dist'] for item in low_entropy_group])
            low_entropy_dx_errors = np.array([item['dx_error'] for item in low_entropy_group])
            low_entropy_dy_errors = np.array([item['dy_error'] for item in low_entropy_group])

            if low_entropy_euc_dists.size > 0:
                print_perf_table(low_entropy_euc_dists, low_entropy_dx_errors, low_entropy_dy_errors)

            low_entropy_threshold_display = low_entropy_group[-1]['entropy']  # ä½ç†µç»„ä¸­æœ€å¤§çš„ç†µå€¼
            print(f"    (æ ·æœ¬ç†µå€¼ <= {low_entropy_threshold_display:.4f})")

            # æ‰“å°ä½ç†µç»„ä¸­éƒ¨åˆ†æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ (é¦–å°¾å‡ ä¸ª)
            num_to_show_low = min(len(low_entropy_group), 7)
            show_first_n_low = (num_to_show_low + 1) // 2
            show_last_n_low = num_to_show_low // 2
            for k, sample_detail in enumerate(low_entropy_group):
                if k < show_first_n_low or k >= len(low_entropy_group) - show_last_n_low:
                    print(
                        f"      æ ·æœ¬ [ç´¢å¼•:{sample_detail['index']}]: ç†µ={sample_detail['entropy']:.4f}, æ¬§æ°è·ç¦»={sample_detail['euc_dist']:.2f}, GT=({sample_detail['gt_offset'][0]:.1f},{sample_detail['gt_offset'][1]:.1f}), Pred=({sample_detail['pred_offset'][0]},{sample_detail['pred_offset'][1]}), dxè¯¯å·®={sample_detail['dx_error']:.2f}, dyè¯¯å·®={sample_detail['dy_error']:.2f}")
                elif k == show_first_n_low and len(low_entropy_group) > num_to_show_low:
                    print("      ...")  # çœç•¥ä¸­é—´çš„æ ·æœ¬

            if apply_ransac_on_low_entropy and len(low_entropy_group) >= min_ransac_samples:
                print(f"\n    --- RANSAC è¯„ä¼° (ä½ç†µç»„) ---")
                src_pts_list = [[item['pred_offset'][1], item['pred_offset'][0]] for item in low_entropy_group]
                dst_pts_list = [[item['gt_offset'][1], item['gt_offset'][0]] for item in low_entropy_group]
                src_pts_np = np.float32(src_pts_list).reshape(-1, 1, 2)
                dst_pts_np = np.float32(dst_pts_list).reshape(-1, 1, 2)

                # M_affine åœ¨å‡½æ•°å¼€å§‹æ—¶å·²åˆå§‹åŒ–ä¸º None
                try:
                    M_affine, inliers_mask_affine = cv2.estimateAffine2D(src_pts_np, dst_pts_np,
                                                                         method=cv2.RANSAC,
                                                                         ransacReprojThreshold=ransac_reproj_threshold,
                                                                         maxIters=2000, confidence=0.99)
                    if M_affine is not None:
                        num_inliers = np.sum(inliers_mask_affine)
                        inlier_ratio = num_inliers / len(low_entropy_group) if len(low_entropy_group) > 0 else 0
                        print(f"      RANSAC (ä»¿å°„å˜æ¢) ç»“æœ:")
                        print(f"        æ€»ç‚¹æ•° (ä½ç†µç»„): {len(low_entropy_group)}")
                        print(f"        å†…ç‚¹æ•°: {num_inliers}")
                        print(f"        å†…ç‚¹æ¯”ä¾‹: {inlier_ratio:.4f}")
                        print(f"        ä¼°è®¡çš„ä»¿å°„å˜æ¢çŸ©é˜µ M (å°†é¢„æµ‹åç§»å˜æ¢è‡³æ¥è¿‘çœŸå®åç§»):\n{M_affine}")
                        if num_inliers > 0:
                            src_inliers = src_pts_np[inliers_mask_affine.ravel() == 1]
                            dst_inliers = dst_pts_np[inliers_mask_affine.ravel() == 1]
                            transformed_src_inliers_raw = cv2.transform(src_inliers, M_affine)
                            transformed_src_inliers_flat = transformed_src_inliers_raw.reshape(-1, 2)
                            dst_inliers_flat = dst_inliers.reshape(-1, 2)
                            reprojection_errors_inliers = np.linalg.norm(
                                transformed_src_inliers_flat - dst_inliers_flat, axis=1)
                            print(f"        å†…ç‚¹å¹³å‡é‡æŠ•å½±è¯¯å·®: {np.mean(reprojection_errors_inliers):.4f} åƒç´ ")
                            print(f"        å†…ç‚¹ä¸­ä½é‡æŠ•å½±è¯¯å·®: {np.median(reprojection_errors_inliers):.4f} åƒç´ ")
                            rmse_inliers_low_entropy = np.sqrt(np.mean(np.square(reprojection_errors_inliers)))
                            print(f"        å†…ç‚¹RMSE (M(é¢„æµ‹åç§») vs çœŸå®åç§»): {rmse_inliers_low_entropy:.4f} åƒç´ ")
                    else:
                        print(f"      RANSAC (ä»¿å°„å˜æ¢) æœªèƒ½ä¼°è®¡å˜æ¢æ¨¡å‹ (ä½ç†µç»„, å¯èƒ½å†…ç‚¹ä¸è¶³æˆ–ç‚¹å…±çº¿ç­‰)ã€‚")
                except cv2.error as e_cv2:
                    print(f"      RANSAC (ä»¿å°„å˜æ¢) æ‰§è¡Œå¤±è´¥ (ä½ç†µç»„, cv2.error): {e_cv2}")
                    if M_affine is not None:
                        print(f"        é”™è¯¯å‘ç”Ÿæ—¶ M_affine çš„å€¼: {M_affine}")
                except Exception as e_ransac:
                    print(f"      RANSAC (ä»¿å°„å˜æ¢) è¯„ä¼°ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯ (ä½ç†µç»„): {e_ransac}")
            elif apply_ransac_on_low_entropy:  # å¦‚æœå¯ç”¨äº†RANSACä½†æ ·æœ¬ä¸è¶³
                print(f"\n    --- RANSAC è¯„ä¼° (ä½ç†µç»„) ---")
                print(
                    f"      æ ·æœ¬æ•° ({len(low_entropy_group)}) å°‘äº RANSAC æ‰€éœ€æœ€å°æ ·æœ¬æ•° ({min_ransac_samples})ï¼Œè·³è¿‡RANSACã€‚")
        else:
            print("    ä½ç†µç»„ä¸­æ²¡æœ‰æ ·æœ¬ã€‚")

        # --- é«˜ç†µç»„è¯„ä¼° ---
        print(f"\n  --- é«˜ç†µç»„ (å‰©ä½™ {len(high_entropy_group)} ä¸ªæ ·æœ¬) ---")
        if high_entropy_group:
            high_entropy_euc_dists = np.array([item['euc_dist'] for item in high_entropy_group])
            high_entropy_dx_errors = np.array([item['dx_error'] for item in high_entropy_group])
            high_entropy_dy_errors = np.array([item['dy_error'] for item in high_entropy_group])
            if high_entropy_euc_dists.size > 0:
                print_perf_table(high_entropy_euc_dists, high_entropy_dx_errors, high_entropy_dy_errors)

            actual_high_entropy_start_threshold = high_entropy_group[0]['entropy'] if high_entropy_group else float(
                'inf')
            if low_entropy_threshold_display != -1.0:  # å¦‚æœä½ç†µç»„å­˜åœ¨ä¸”æœ‰é˜ˆå€¼
                print(
                    f"    (æ ·æœ¬ç†µå€¼ > {low_entropy_threshold_display:.4f}, å®é™…èµ·å§‹äº {actual_high_entropy_start_threshold:.4f})")
            else:  # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½åœ¨é«˜ç†µç»„
                print(f"    (æ‰€æœ‰æ ·æœ¬å‡åœ¨æ­¤ç»„, å®é™…èµ·å§‹ç†µå€¼: {actual_high_entropy_start_threshold:.4f})")

            # æ‰“å°é«˜ç†µç»„ä¸­éƒ¨åˆ†æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            num_to_show_high = min(len(high_entropy_group), 7)
            show_first_n_high = (num_to_show_high + 1) // 2
            show_last_n_high = num_to_show_high // 2
            for k, sample_detail in enumerate(high_entropy_group):
                if k < show_first_n_high or k >= len(high_entropy_group) - show_last_n_high:
                    print(
                        f"      æ ·æœ¬ [ç´¢å¼•:{sample_detail['index']}]: ç†µ={sample_detail['entropy']:.4f}, æ¬§æ°è·ç¦»={sample_detail['euc_dist']:.2f}, GT=({sample_detail['gt_offset'][0]:.1f},{sample_detail['gt_offset'][1]:.1f}), Pred=({sample_detail['pred_offset'][0]},{sample_detail['pred_offset'][1]}), dxè¯¯å·®={sample_detail['dx_error']:.2f}, dyè¯¯å·®={sample_detail['dy_error']:.2f}")
                elif k == show_first_n_high and len(high_entropy_group) > num_to_show_high:
                    print("      ...")
        else:
            print("    é«˜ç†µç»„ä¸­æ²¡æœ‰æ ·æœ¬ã€‚")

        # --- ç»˜åˆ¶ç›´æ–¹å›¾ (ä»…åœ¨ç†µæ¨¡å¼ä¸‹) ---
        all_entropies_np = np.array([item['entropy'] for item in sorted_samples_by_entropy])
        if all_entropies_np.size == 0:
            print("æ²¡æœ‰æœ‰æ•ˆçš„ç†µå€¼æ•°æ®ç”¨äºç»˜åˆ¶ç›´æ–¹å›¾ã€‚")
        else:
            print("\nğŸ“Š å¼€å§‹ç»˜åˆ¶ç†µå€¼ç›´æ–¹å›¾...")
            plt.figure(figsize=(12, 7))
            plt.hist(all_entropies_np, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)
            plt.title(f'{model_name_for_log} çš„çƒ­å›¾ç†µå€¼åˆ†å¸ƒ\næ€»æ ·æœ¬æ•°: {num_total_samples}', fontsize=14)
            plt.xlabel('ç†µ (æ¯”ç‰¹)', fontsize=12)
            plt.ylabel('é¢‘ç‡', fontsize=12)
            plt.grid(axis='y', linestyle='--', alpha=0.7)

            # æ ‡è®°ä½ç†µ/é«˜ç†µåˆ†å‰²çº¿
            if low_entropy_group and high_entropy_group:  # åªæœ‰å½“ä¸¤ç»„éƒ½å­˜åœ¨æ—¶ï¼Œåˆ†å‰²çº¿æ‰æœ‰æ„ä¹‰
                threshold_value_for_plot = low_entropy_group[-1]['entropy']
                plt.axvline(threshold_value_for_plot, color='dodgerblue', linestyle='dashed', linewidth=2,
                            label=f'ä½/é«˜ç†µé˜ˆå€¼ (æœ€ä½ {entropy_percentile_for_selection}%)\nç†µ $\leq$ {threshold_value_for_plot:.2f}')
            elif low_entropy_group and not high_entropy_group:  # æ‰€æœ‰æ ·æœ¬éƒ½åœ¨ä½ç†µç»„
                plt.text(0.98, 0.95, 'æ‰€æœ‰æ ·æœ¬å‡åœ¨ä½ç†µç»„', ha='right', va='top',
                         transform=plt.gca().transAxes, color='dodgerblue')
            elif not low_entropy_group and high_entropy_group:  # æ‰€æœ‰æ ·æœ¬éƒ½åœ¨é«˜ç†µç»„
                plt.text(0.98, 0.95, 'æ‰€æœ‰æ ·æœ¬å‡åœ¨é«˜ç†µç»„', ha='right', va='top',
                         transform=plt.gca().transAxes, color='darkorange')

            # æ ‡è®°ç”¨äºæ—¥å¿—è®°å½•çš„é«˜ç†µç™¾åˆ†ä½ç‚¹
            if 0 < log_highest_entropy_samples_percentile < 100:
                log_percentile_value_for_plot = np.percentile(all_entropies_np, log_highest_entropy_samples_percentile)
                plt.axvline(log_percentile_value_for_plot, color='darkviolet', linestyle='dotted', linewidth=2,
                            label=f'{log_highest_entropy_samples_percentile}ç™¾åˆ†ä½ç†µ\nå€¼ $\geq$ {log_percentile_value_for_plot:.2f}')

            if plt.gca().has_data():  # ç¡®ä¿å›¾ä¾‹æœ‰å†…å®¹å¯æ˜¾ç¤º
                plt.legend(fontsize='small', loc='best')
            plt.tight_layout()  # è°ƒæ•´å¸ƒå±€ä»¥é˜²æ­¢æ ‡ç­¾é‡å 
            plot_filename = os.path.join(model_output_dir, f"entropy_histogram_grouped.png")
            try:
                plt.savefig(plot_filename)
                print(f"âœ… ç†µå€¼ç›´æ–¹å›¾å·²ä¿å­˜åˆ°: {plot_filename}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•ä¿å­˜ç†µå€¼ç›´æ–¹å›¾: {e}")
            plt.close()  # å…³é—­å›¾åƒä»¥é‡Šæ”¾å†…å­˜

    M_to_return = None  # åˆå§‹åŒ–è¦è¿”å›çš„å˜æ¢çŸ©é˜µ
    if run_ransac_only_on_all_samples_mode:
        print(f"\n--- ğŸ›¡ï¸ æ¨¡å‹ '{model_name_for_log}' çš„ä»…RANSACè¯„ä¼°ç»“æŸ ---")
        M_to_return = M_affine_all  # åœ¨ä»…RANSACæ¨¡å¼ä¸‹ï¼Œè¿”å›åœ¨æ‰€æœ‰æ ·æœ¬ä¸Šè®¡ç®—çš„çŸ©é˜µ
    else:
        print(f"\n--- ğŸ›¡ï¸ æ¨¡å‹ '{model_name_for_log}' çš„åŸºäºç†µå€¼çš„è¯„ä¼°ç»“æŸ ---")
        if apply_ransac_on_low_entropy and M_affine is not None:  # æ£€æŸ¥åœ¨ç†µæ¨¡å¼ä¸‹æ˜¯å¦è®¡ç®—äº†M_affine
            M_to_return = M_affine  # åœ¨ç†µæ¨¡å¼ä¸‹ï¼Œå¦‚æœåº”ç”¨äº†RANSACå¹¶æˆåŠŸï¼Œåˆ™è¿”å›ä½ç†µç»„çš„çŸ©é˜µ
        # å¦åˆ™ M_to_return ä¿æŒä¸º None

    return sample_metrics_data, M_to_return
