# main.py
import os
import tensorflow as tf
import numpy as np
import random
import traceback
import gc
import sys
from tqdm import tqdm
import SarOptMatch  # ä½ çš„åŒ…
from SarOptMatch.train_modules import BestModelSaver, evaluate_best_model, plot_training_curve
from SarOptMatch.log import setup_logger
from tensorflow.keras import backend as K
import pickle
import time
import glob
import matplotlib.pyplot as plt
import matplotlib

# =================================================================
# <<< æ–°å¢ä»£ç ï¼šåˆå§‹åŒ–æ—¶é—´å’Œå†…å­˜ç›‘æ§ >>>
# =================================================================
script_start_time = time.time()
psutil_available = False
try:
    import psutil

    process = psutil.Process(os.getpid())
    psutil_available = True
    # è®°å½•åˆå§‹å†…å­˜ï¼Œç”¨äºè®¡ç®—å‡€å¢é‡
    start_memory_rss = process.memory_info().rss
    print(f"âœ… psutilå¯¼å…¥æˆåŠŸï¼Œå·²å¼€å§‹ç›‘æ§å†…å­˜ã€‚åˆå§‹RSS: {start_memory_rss / 1024 ** 2:.2f} MB")
except ImportError:
    print("âš ï¸ è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ 'psutil' åº“ã€‚å°†æ— æ³•ç»Ÿè®¡å†…å­˜ä½¿ç”¨æƒ…å†µã€‚")
    print("   è¯·é€šè¿‡ 'pip install psutil' å‘½ä»¤å®‰è£…ã€‚")
# =================================================================


# =================================================================
#  <<< æ–°å¢ä»£ç ï¼šè§£å†³ Matplotlib ä¸­æ–‡ä¹±ç é—®é¢˜ >>>
# =================================================================
# æ¨èä½¿ç”¨ 'SimHei' (é»‘ä½“)ï¼Œå®ƒåœ¨å¤§å¤šæ•° Windows ç³»ç»Ÿä¸­éƒ½å¯ç”¨
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
# è§£å†³ä¿å­˜å›¾åƒæ—¶è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
matplotlib.rcParams['axes.unicode_minus'] = False
# =================================================================

# =================================================================
# <<< æ–°å¢ä»£ç ï¼šä½¿ç”¨ try...finally ç»“æ„åŒ…è£¹ä¸»é€»è¾‘ >>>
# =================================================================
try:
    try:
        from SarOptMatch.evaluate_with_confidence import run_confidence_evaluation_with_entropy
    except ImportError:
        print(
            "âŒ é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ run_confidence_evaluation_with_entropyã€‚è¯·ç¡®ä¿ SarOptMatch.evaluate_with_confidence.py æ–‡ä»¶å­˜åœ¨ä¸”å‡½æ•°å·²æ­£ç¡®å®šä¹‰ã€‚")


        def run_confidence_evaluation_with_entropy(*args, **kwargs):
            print("âš ï¸ è­¦å‘Šï¼šrun_confidence_evaluation_with_entropy æœªæˆåŠŸå¯¼å…¥ï¼Œè·³è¿‡åŸºäºç†µå€¼çš„è¯„ä¼°ã€‚")
            return None, None

    try:
        from SarOptMatch.net_shift_visualizer import visualize_large_images_with_net_ransac_shift

        print("âœ… Successfully imported visualize_large_images_with_net_ransac_shift from SarOptMatch.")
        # ... (ä¿ç•™æ‚¨çš„è°ƒè¯•æ‰“å°)
    except ImportError as e_viz:
        print(f"âš ï¸ Warning: Could not import visualize_large_images_with_net_ransac_shift from SarOptMatch: {e_viz}")
        print("   Skipping large image net shift visualization.")


        def visualize_large_images_with_net_ransac_shift(*args, **kwargs):
            pass

    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… {len(gpus)} GPU(s) found and memory growth set.")
        except RuntimeError as e:
            print(f"GPUè®¾ç½®é”™è¯¯: {e}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°GPUï¼Œå°†ä½¿ç”¨CPUã€‚")

    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

    TRAIN_MODEL = False
    PRETRAINED_WEIGHTS_DIR = "weights"
    MODEL_OPERATIONS_DIR = "lambda_models"
    ENTROPY_EVALUATION_OUTPUT_DIR = "evaluation_results_grouped_by_entropy"
    ORIGINAL_MASTER_DIR = r"E:\zhanghan\master_ims4"
    ORIGINAL_SLAVE_DIR = r"E:\zhanghan\slave_ims4"

    main_dataset_paths = [r'E:\OSdataset\OSdataset-5\OSdataset\256']
    main_ims_per_folder_list = [9623]

    EXTERNAL_VALIDATION_DATASET_PATH = r'E:\zhanghan\qietu_4\offset_pos32'
    EXTERNAL_VALIDATION_IMS_PER_FOLDER = 8370
    EXTERNAL_VALIDATION_BATCH_SIZE = 4

    success_datasets = []
    failed_datasets = []
    n_filters_list = [32]
    ENTROPY_PERCENTILE_FOR_SELECTION = 20
    LOG_HIGHEST_ENTROPY_PERCENTILE = 100 - ENTROPY_PERCENTILE_FOR_SELECTION

    # NEW: å®šä¹‰ç»Ÿä¸€çš„é‡‡æ ·ç­–ç•¥åˆ—è¡¨
    # 'conv': ç¼–ç å™¨ç”¨å·ç§¯ä¸‹é‡‡æ ·ï¼Œè§£ç å™¨ç”¨è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
    # 'interp_pool': ç¼–ç å™¨ç”¨æ± åŒ–ä¸‹é‡‡æ ·ï¼Œè§£ç å™¨ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
    SAMPLING_STRATEGIES = ['conv']

    # ä¸»æ•°æ®é›†å¤„ç†å¾ªç¯
    for dataset_path, ims_per_folder in tqdm(zip(main_dataset_paths, main_ims_per_folder_list),
                                             total=len(main_dataset_paths),
                                             desc="ğŸ› ï¸ ä¸»æ•°æ®é›†å¤„ç†è¿›åº¦",
                                             file=sys.stdout,
                                             leave=True):
        dataset_name = os.path.basename(dataset_path)
        logger = None

        for n_filters in n_filters_list:
            # NEW: å¾ªç¯éå†ä¸åŒçš„ç»Ÿä¸€é‡‡æ ·ç­–ç•¥
            for sampling_strategy_choice in SAMPLING_STRATEGIES:
                # æ ¹æ®ç­–ç•¥ç¡®å®šç¼–ç å™¨å’Œè§£ç å™¨çš„å…·ä½“æ–¹æ³•
                if sampling_strategy_choice == 'conv':
                    encoder_use_conv_down = True
                    decoder_upsampling_method = 'transpose_conv'
                    strategy_str_for_id = "conv_all"  # ç”¨äºæ–‡ä»¶åå’Œæ—¥å¿—
                elif sampling_strategy_choice == 'interp_pool':
                    encoder_use_conv_down = False  # ä½¿ç”¨æ± åŒ–
                    decoder_upsampling_method = 'bilinear'
                    strategy_str_for_id = "interp_pool_all"
                else:
                    print(f"âš ï¸ æœªçŸ¥çš„é‡‡æ ·ç­–ç•¥: {sampling_strategy_choice}ã€‚è·³è¿‡æ­¤ç­–ç•¥ã€‚")
                    continue

                # MODIFIED: æ›´æ–° current_model_id ä»¥åæ˜ ç»Ÿä¸€çš„é‡‡æ ·ç­–ç•¥
                current_model_id = f"{dataset_name}_nfilt{n_filters}_strat_{strategy_str_for_id}"
                model_ready_for_evaluation = False

                if logger is not None and hasattr(logger, 'handlers'):
                    for handler in logger.handlers[:]:
                        handler.close()
                        logger.removeHandler(handler)
                logger = setup_logger(current_model_id)
                print(
                    f"\nğŸ” æ­£åœ¨å¤„ç†ä¸»æ•°æ®é›†: {dataset_name} | æ¯ä¸ªæ–‡ä»¶å¤¹å›¾åƒæ•°: {ims_per_folder} | n_filters = {n_filters} | é‡‡æ ·ç­–ç•¥: {sampling_strategy_choice} (EncDownConv: {encoder_use_conv_down}, DecUp: {decoder_upsampling_method})")
                print(f"æ—¥å¿—æ–‡ä»¶: logs/{current_model_id}.log")

                try:
                    print(f"--- ğŸ“‚ åŠ è½½å’Œåˆ†å‰²ä¸»æ•°æ®é›†: {dataset_name} ---")
                    sar_files, opt_files, offsets = SarOptMatch.dataset.sen1_2(
                        data_path=dataset_path,
                        seed=seed,
                        ims_per_folder=ims_per_folder
                    )

                    TRAIN_BATCH_SIZE = 4
                    training_data, main_validation_data, main_validation_dataRGB = SarOptMatch.dataset.split_data(
                        sar_files, opt_files, offsets,
                        batch_size=TRAIN_BATCH_SIZE,
                        seed=seed,
                        masking_strategy="unet"
                    )
                    print(f"ä¸»æ•°æ®é›†: {len(sar_files)} ä¸ªæ ·æœ¬. è®­ç»ƒé›†å’Œä¸»éªŒè¯é›† (7:3) å·²åˆ›å»ºã€‚")

                    matcher_init_config = {
                        'backbone': 'UnetPlus',
                        'n_filters': n_filters,
                        'activation': 'relu',
                        'use_conv_downsampling': encoder_use_conv_down,  # <--- NEW: ä»ç­–ç•¥ä¸­è·å–
                        'decoder_upsampling_method': decoder_upsampling_method,  # <--- NEW: ä»ç­–ç•¥ä¸­è·å–
                        'model_name': current_model_id
                        # å¯ä»¥æ·»åŠ  star_mlp_ratio, star_drop_path_rate ç­‰å…¶ä»– UnetPlus å‚æ•°
                    }
                    matcher = SarOptMatch.architectures.SAR_opt_Matcher(**matcher_init_config)

                    matcher.create_model(
                        reference_im_shape=(256, 256, 1),
                        floating_im_shape=(192, 192, 1),
                        model_id_for_name=current_model_id
                    )

                    history_object = None
                    best_model_saver = None

                    if TRAIN_MODEL:
                        print(f"æ¨¡å¼: ğŸŸ¢ è®­ç»ƒæ¨¡å¼å·²å¯ç”¨ (TRAIN_MODEL = True)")
                        print(
                            f"ä½¿ç”¨ n_filters = {n_filters}, é‡‡æ ·ç­–ç•¥ = {sampling_strategy_choice} åœ¨ä¸»æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒ (Batch Size: {TRAIN_BATCH_SIZE})...")
                        best_model_saver = BestModelSaver(main_validation_data, matcher, dataset_name=current_model_id)
                        print("ğŸš€ å¼€å§‹åœ¨ä¸»æ•°æ®é›†ä¸Šè®­ç»ƒ...")
                        history_object, early_stopped = matcher.train(
                            training_data,
                            main_validation_data,
                            epochs=1,
                            callbacks=[best_model_saver]
                        )
                        print("ğŸ ä¸»æ•°æ®é›†è®­ç»ƒå®Œæˆ.")
                        if os.path.exists(os.path.join(MODEL_OPERATIONS_DIR, f"{current_model_id}_best_model.h5")):
                            model_ready_for_evaluation = True
                            print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹åº”å·²ä¿å­˜åˆ° {MODEL_OPERATIONS_DIR} å¹¶åŠ è½½åˆ° matcher.modelã€‚")
                        else:
                            print(
                                f"âš ï¸ è®­ç»ƒå®Œæˆï¼Œä½†æœªåœ¨ {MODEL_OPERATIONS_DIR} æ‰¾åˆ°é¢„æœŸçš„æœ€ä½³æ¨¡å‹æ–‡ä»¶ {current_model_id}_best_model.h5ã€‚è¯„ä¼°å¯èƒ½ä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£çš„æ¨¡å‹ã€‚")
                            model_ready_for_evaluation = True

                        history_save_dir = PRETRAINED_WEIGHTS_DIR
                        os.makedirs(history_save_dir, exist_ok=True)
                        history_save_path = os.path.join(history_save_dir, f"{current_model_id}_history.pkl")
                        try:
                            with open(history_save_path, 'wb') as f:
                                if history_object and hasattr(history_object, 'history'):
                                    pickle.dump(history_object.history, f)
                                    print(f"âœ… è®­ç»ƒå†å² (å­—å…¸) å·²ä¿å­˜åˆ°: {history_save_path}")
                        except Exception as e_hist_save:
                            print(f"âš ï¸ ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e_hist_save}")

                        if history_object and best_model_saver:
                            score_history_to_plot = best_model_saver.score_history if hasattr(best_model_saver,
                                                                                              'score_history') else None
                            plot_training_curve(
                                model_name=current_model_id,
                                history_object=history_object,
                                score_history_list=score_history_to_plot
                            )

                    else:  # TRAIN_MODEL is False
                        print(f"æ¨¡å¼: ğŸŸ¡ åŠ è½½æ¨¡å¼å·²å¯ç”¨ (TRAIN_MODEL = False)")
                        best_model_filename = f"{current_model_id}_best_model.h5"
                        weights_path = os.path.join(MODEL_OPERATIONS_DIR, best_model_filename)
                        if os.path.exists(weights_path):
                            try:
                                print(f"â³ æ­£åœ¨ä» {weights_path} åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ° matcher.model...")
                                matcher.model.load_weights(weights_path)
                                print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡åˆ° matcher.modelã€‚")
                                model_ready_for_evaluation = True
                            except Exception as e_load:
                                print(f"âš ï¸ åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ° matcher.model å¤±è´¥: {e_load}")
                        else:
                            print(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡æ–‡ä»¶: {weights_path} (åœ¨ {MODEL_OPERATIONS_DIR} ä¸­)")

                    if not model_ready_for_evaluation:
                        print(f"ğŸ”´ æ¨¡å‹ ({current_model_id}) æœªèƒ½æˆåŠŸå‡†å¤‡ (è®­ç»ƒæˆ–åŠ è½½)ã€‚è·³è¿‡æ­¤é…ç½®çš„æ‰€æœ‰è¯„ä¼°æ­¥éª¤ã€‚")
                        failed_datasets.append(f"{current_model_id} (æ¨¡å‹å‡†å¤‡å¤±è´¥)")
                        continue

                    print(
                        f"\n--- ğŸ“ˆ å¼€å§‹åœ¨ ä¸»éªŒè¯é›† ({dataset_name}) ä¸Šè¿›è¡Œè¯„ä¼° (evaluate_best_modelå°†ä»å†…éƒ¨åŠ è½½æ¨¡å‹) ---")
                    try:
                        evaluate_best_model(matcher, main_validation_data, main_validation_dataRGB, current_model_id,
                                            extract_features=False)

                        if matcher.model:
                            main_val_entropy_log_id = f"{current_model_id}_main_val"
                            print(
                                f"\n--- ğŸ›¡ï¸ è°ƒç”¨åŸºäºç†µå€¼çš„è¯„ä¼° (ä¸»éªŒè¯é›†: {dataset_name}, ä½¿ç”¨ matcher.model, Log ID: {main_val_entropy_log_id}) ---")
                            run_confidence_evaluation_with_entropy(
                                model_to_evaluate=matcher.model,
                                validation_data_for_confidence=main_validation_data,
                                model_name_for_log=main_val_entropy_log_id,
                                entropy_percentile_for_selection=ENTROPY_PERCENTILE_FOR_SELECTION,
                                log_highest_entropy_samples_percentile=LOG_HIGHEST_ENTROPY_PERCENTILE,
                                output_dir=ENTROPY_EVALUATION_OUTPUT_DIR,
                                apply_ransac_on_low_entropy=True,
                                ransac_reproj_threshold=3.0,
                                min_ransac_samples=4,
                                run_ransac_only_on_all_samples_mode=False
                            )
                        else:
                            print(f"âš ï¸ matcher.model æœªå®šä¹‰æˆ–æ— æ•ˆï¼Œè·³è¿‡åœ¨ä¸»éªŒè¯é›†ä¸Šçš„åŸºäºç†µå€¼çš„è¯„ä¼°ã€‚")
                    except Exception as e_eval_main:
                        print(f"âš ï¸ åœ¨ä¸»éªŒè¯é›† ({dataset_name}) ä¸Šçš„è¯„ä¼°å¤±è´¥: {str(e_eval_main)}")
                        traceback.print_exc()

                    if EXTERNAL_VALIDATION_DATASET_PATH and os.path.exists(EXTERNAL_VALIDATION_DATASET_PATH):
                        external_dataset_name = os.path.basename(EXTERNAL_VALIDATION_DATASET_PATH)
                        external_model_id_for_loading = current_model_id

                        print(
                            f"\n\n--- âœ¨ [è®¡æ—¶å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹å¤„ç†å¤–éƒ¨éªŒè¯æ•°æ®é›†: {external_dataset_name} (ä½¿ç”¨æ¨¡å‹: {external_model_id_for_loading}) ---")
                        start_time_external_eval = time.time()
                        try:
                            print(f"--- ğŸ“‚ åŠ è½½å¤–éƒ¨éªŒè¯æ•°æ®é›†: {external_dataset_name} ---")
                            ext_sar_files, ext_opt_files, ext_offsets = SarOptMatch.dataset.sen1_2(
                                data_path=EXTERNAL_VALIDATION_DATASET_PATH,
                                seed=seed,
                                ims_per_folder=EXTERNAL_VALIDATION_IMS_PER_FOLDER
                            )
                            print(f"å¤–éƒ¨éªŒè¯æ•°æ®é›†: {len(ext_sar_files)} ä¸ªæ ·æœ¬å°†ç”¨äºè¯„ä¼°ã€‚")

                            if ext_sar_files.size == 0:
                                print(f"âš ï¸ å¤–éƒ¨éªŒè¯æ•°æ®é›† {external_dataset_name} ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°ã€‚")
                            else:
                                external_val_data = SarOptMatch.dataset.files_to_dataset(
                                    ext_opt_files, ext_sar_files, ext_offsets,
                                    masking_strategy="unet",
                                    batch_size=EXTERNAL_VALIDATION_BATCH_SIZE,
                                    convert_to_grayscale=True
                                )
                                print(
                                    f"\n--- ğŸ“ˆ å¼€å§‹åœ¨ å¤–éƒ¨éªŒè¯é›† ({external_dataset_name}) ä¸Šè¿›è¡Œè¯„ä¼° (evaluate_best_modelå°†ä»å†…éƒ¨åŠ è½½æ¨¡å‹) ---")
                                evaluate_best_model(matcher, external_val_data, None,
                                                    external_model_id_for_loading,
                                                    extract_features=False)

                                if matcher.model:
                                    # --- å…³é”®ä¿®å¤ï¼šä¸ºå…¨å±€RANSACå’Œç†µå€¼è¯„ä¼°åˆ›å»ºåŒ…å«æ‰€æœ‰æ ·æœ¬çš„å•ä¸ªæ‰¹æ¬¡æ•°æ®é›† ---
                                    print(
                                        f"   å‡†å¤‡å¤–éƒ¨éªŒè¯é›†å…¨å±€è¯„ä¼°æ•°æ®ï¼šåˆ›å»ºåŒ…å«å…¨éƒ¨ {len(ext_sar_files)} ä¸ªæ ·æœ¬çš„å•ä¸ªæ‰¹æ¬¡...")
                                    external_val_data_for_global_eval = SarOptMatch.dataset.files_to_dataset(
                                        ext_opt_files, ext_sar_files, ext_offsets,
                                        masking_strategy="unet",
                                        batch_size=EXTERNAL_VALIDATION_BATCH_SIZE,  # å°†æ‰€æœ‰æ ·æœ¬æ”¾å…¥ä¸€ä¸ªæ‰¹æ¬¡
                                        convert_to_grayscale=True,

                                    )
                                    print("   å¤–éƒ¨éªŒè¯é›†å•æ‰¹æ¬¡æ•°æ®å‡†å¤‡å®Œæˆã€‚")
                                    # --- ä¿®å¤ç»“æŸ ---
                                    ext_val_entropy_log_id = f"{current_model_id}_on_ext_{external_dataset_name}"
                                    print(
                                        f"\n--- ğŸ›¡ï¸ è°ƒç”¨åŸºäºç†µå€¼çš„è¯„ä¼° (å¤–éƒ¨éªŒè¯é›†: {external_dataset_name}, ä½¿ç”¨ matcher.model, Log ID: {ext_val_entropy_log_id}) ---")
                                    run_confidence_evaluation_with_entropy(
                                        model_to_evaluate=matcher.model,
                                        validation_data_for_confidence=external_val_data_for_global_eval,
                                        # <--- ä½¿ç”¨æ–°åˆ›å»ºçš„æ•°æ®é›†
                                        model_name_for_log=ext_val_entropy_log_id,
                                        entropy_percentile_for_selection=ENTROPY_PERCENTILE_FOR_SELECTION,
                                        log_highest_entropy_samples_percentile=LOG_HIGHEST_ENTROPY_PERCENTILE,
                                        output_dir=ENTROPY_EVALUATION_OUTPUT_DIR,
                                        apply_ransac_on_low_entropy=True,
                                        ransac_reproj_threshold=3.0,
                                        min_ransac_samples=4,
                                        run_ransac_only_on_all_samples_mode=False
                                    )

                                    base_id_for_ransac = current_model_id.replace(dataset_name, "ds")
                                    ext_name_short = external_dataset_name[:4]
                                    # MODIFIED: RANSAC æ—¥å¿— ID ç°åœ¨ä¹Ÿåæ˜ ç»Ÿä¸€çš„é‡‡æ ·ç­–ç•¥
                                    ext_val_ransac_only_log_id = f"{base_id_for_ransac}_ext_{ext_name_short}_{strategy_str_for_id}_RANSAC"

                                    print(
                                        f"\n--- ğŸ›¡ï¸âœ¨ è°ƒç”¨ä»…RANSACè¯„ä¼° (å¤–éƒ¨éªŒè¯é›†: {external_dataset_name}, ä½¿ç”¨ matcher.model, Log ID: {ext_val_ransac_only_log_id}) ---")
                                    start_time_ransac_only_eval = time.time()
                                    # --- å…³é”®ä¿®å¤ï¼šä¸ºå…¨å±€RANSACåˆ›å»ºåŒ…å«æ‰€æœ‰æ ·æœ¬çš„å•ä¸ªæ‰¹æ¬¡æ•°æ®é›† ---
                                    print(
                                        f"   å‡†å¤‡å…¨å±€RANSACæ•°æ®ï¼šåˆ›å»ºåŒ…å«å…¨éƒ¨ {len(ext_sar_files)} ä¸ªæ ·æœ¬çš„å•ä¸ªæ‰¹æ¬¡...")
                                    external_val_data_for_ransac_only = SarOptMatch.dataset.files_to_dataset(
                                        ext_opt_files, ext_sar_files, ext_offsets,
                                        masking_strategy="unet",
                                        batch_size=len(ext_sar_files),  # å°†æ‰€æœ‰æ ·æœ¬æ”¾å…¥ä¸€ä¸ªæ‰¹æ¬¡
                                        convert_to_grayscale=True,

                                    )
                                    # --- ä¿®å¤ç»“æŸ ---

                                    collected_samples_ransac_only, M_affine_global_from_ransac = run_confidence_evaluation_with_entropy(
                                        model_to_evaluate=matcher.model,
                                        validation_data_for_confidence=external_val_data_for_global_eval,
                                        # <--- å¤ç”¨è¿™ä¸ªæ•°æ®é›†
                                        model_name_for_log=ext_val_ransac_only_log_id,
                                        entropy_percentile_for_selection=0,
                                        log_highest_entropy_samples_percentile=0,
                                        apply_ransac_on_low_entropy=False,
                                        ransac_reproj_threshold=3.0,
                                        min_ransac_samples=4,
                                        run_ransac_only_on_all_samples_mode=True,
                                        output_dir=ENTROPY_EVALUATION_OUTPUT_DIR
                                    )
                                    end_time_ransac_only_eval = time.time()
                                    duration_ransac_only_eval = end_time_ransac_only_eval - start_time_ransac_only_eval
                                    print(
                                        f"--- â±ï¸âœ¨ ä»…RANSACè¯„ä¼° ({ext_val_ransac_only_log_id}) è€—æ—¶: {duration_ransac_only_eval:.2f} ç§’ ({duration_ransac_only_eval / 60:.2f} åˆ†é’Ÿ) ---")

                                    if M_affine_global_from_ransac is not None:
                                        print(
                                            f"\n--- ğŸ’  å‡†å¤‡ä½¿ç”¨Net RANSAC Shiftå¯è§†åŒ–åŸå§‹å¤§å›¾ (åŸºäºæ¨¡å‹ {current_model_id}) ---")
                                        try:
                                            original_master_tifs = sorted(
                                                glob.glob(os.path.join(ORIGINAL_MASTER_DIR, "optical_*.tif")))
                                            original_slave_tifs = sorted(
                                                glob.glob(os.path.join(ORIGINAL_SLAVE_DIR, "sar_*.tif")))

                                            if not original_master_tifs or not original_slave_tifs:
                                                print("âš ï¸ æ— æ³•åœ¨æŒ‡å®šçš„åŸå§‹å¤§å›¾ç›®å½•ä¸­æ‰¾åˆ° .tif æ–‡ä»¶ã€‚è·³è¿‡å¤§å›¾å¯è§†åŒ–ã€‚")
                                            else:
                                                idx_of_original_pair = 0
                                                if idx_of_original_pair < len(original_master_tifs) and \
                                                        idx_of_original_pair < len(original_slave_tifs):
                                                    original_large_opt_path = original_master_tifs[idx_of_original_pair]
                                                    original_large_sar_path = original_slave_tifs[idx_of_original_pair]
                                                    if os.path.exists(original_large_opt_path) and os.path.exists(
                                                            original_large_sar_path):
                                                        artificial_offset_xy_cropping = (32, 32)
                                                        output_subdir_for_viz = os.path.join(
                                                            ENTROPY_EVALUATION_OUTPUT_DIR,
                                                            ext_val_ransac_only_log_id)
                                                        os.makedirs(output_subdir_for_viz, exist_ok=True)
                                                        # MODIFIED: å¯è§†åŒ–æ–‡ä»¶åä¹Ÿåæ˜ ç»Ÿä¸€çš„é‡‡æ ·ç­–ç•¥
                                                        large_img_viz_filename = f"large_img_net_shift_viz_strat_{strategy_str_for_id}.png"
                                                        large_img_viz_full_path = os.path.join(output_subdir_for_viz,
                                                                                               large_img_viz_filename)
                                                        visualize_large_images_with_net_ransac_shift(
                                                            original_optical_large_img_path=original_large_opt_path,
                                                            original_sar_large_img_path=original_large_sar_path,
                                                            M_affine_global_ransac=M_affine_global_from_ransac,
                                                            artificial_offset_xy=artificial_offset_xy_cropping,
                                                            output_visualization_path=large_img_viz_full_path,
                                                            checkerboard_block_size=128
                                                        )
                                                    else:
                                                        print(f"âš ï¸ é€‰å®šçš„åŸå§‹å¤§å›¾è·¯å¾„ä¸å­˜åœ¨ã€‚è·³è¿‡å¤§å›¾å¯è§†åŒ–ã€‚")
                                                else:
                                                    print(
                                                        f"âš ï¸ ç´¢å¼• {idx_of_original_pair} è¶…å‡ºåŸå§‹å¤§å›¾æ–‡ä»¶åˆ—è¡¨èŒƒå›´ã€‚è·³è¿‡å¤§å›¾å¯è§†åŒ–ã€‚")
                                        except Exception as e_large_viz:
                                            print(f"âš ï¸ è°ƒç”¨å¤§å›¾å¯è§†åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e_large_viz}")
                                            traceback.print_exc()
                                    else:
                                        print("âš ï¸ æœªèƒ½ä»RANSACè¯„ä¼°ä¸­è·å–æœ‰æ•ˆçš„å…¨å±€ä»¿å°„çŸ©é˜µã€‚è·³è¿‡å¤§å›¾å¯è§†åŒ–ã€‚")
                                else:
                                    print(f"âš ï¸ matcher.model æœªå®šä¹‰æˆ–æ— æ•ˆï¼Œè·³è¿‡åœ¨å¤–éƒ¨éªŒè¯é›†ä¸Šçš„åŸºäºç†µå€¼çš„è¯„ä¼°ã€‚")
                        except Exception as e_eval_ext:
                            print(f"âš ï¸ åœ¨å¤–éƒ¨éªŒè¯é›† ({external_dataset_name}) ä¸Šçš„è¯„ä¼°å¤±è´¥: {str(e_eval_ext)}")
                            traceback.print_exc()
                        finally:
                            end_time_external_eval = time.time()
                            duration_external_eval = end_time_external_eval - start_time_external_eval
                            print(
                                f"--- âœ¨ [è®¡æ—¶ç»“æŸ: {time.strftime('%Y-%m-%d %H:%M:%S')}] å¤–éƒ¨éªŒè¯æ•°æ®é›† ({external_dataset_name}) å¤„ç†å®Œæˆã€‚")
                            print(
                                f"--- â±ï¸  å¤–éƒ¨éªŒè¯æ•°æ®é›† ({external_dataset_name}) æ€»è€—æ—¶ (åŒ…æ‹¬æ‰€æœ‰è¯„ä¼°ç±»å‹): {duration_external_eval:.2f} ç§’ ({duration_external_eval / 60:.2f} åˆ†é’Ÿ) ---")
                    elif EXTERNAL_VALIDATION_DATASET_PATH:
                        print(f"âš ï¸ æœªæ‰¾åˆ°å¤–éƒ¨éªŒè¯æ•°æ®é›†è·¯å¾„: {EXTERNAL_VALIDATION_DATASET_PATH}ï¼Œè·³è¿‡å¤–éƒ¨éªŒè¯ã€‚")

                    success_datasets.append(current_model_id)
                except Exception as e_main_loop:
                    print(
                        f"âŒ å‡ºé”™å•¦ï¼ä¸»æ•°æ®é›† {dataset_name} (n_filters={n_filters}, strategy={sampling_strategy_choice}) å¤„ç†å‡ºç°é”™è¯¯ï¼š{str(e_main_loop)}")
                    traceback.print_exc()
                    failed_datasets.append(f"{current_model_id} (ä¸»å¾ªç¯å¼‚å¸¸)")
                finally:
                    if logger is not None and hasattr(logger, 'handlers'):
                        for handler in logger.handlers[:]:
                            handler.close()
                            logger.removeHandler(handler)
                    K.clear_session()
                    gc.collect()
                    print(f"--- âœ… ä¸»æ•°æ®é›†å¤„ç†å®Œæˆ: {current_model_id} ---")

        if logger is not None and hasattr(logger, 'handlers') and logger.handlers:
            for handler in logger.handlers[:]:
                handler.close()
                logger.removeHandler(handler)
            # MODIFIED: ç¡®ä¿åœ¨æœ€å¤–å±‚å¾ªç¯çš„æœ€åå…³é—­ logger
            print(
                f"å…³é—­äº†ä¸»æ•°æ®é›† {dataset_name} (n_filters={n_filters_list[-1]}, strategy={SAMPLING_STRATEGIES[-1]}) çš„loggerã€‚")

    print("\n\n=== ğŸ å¤„ç†æ€»ç»“ ğŸ ===")
    print(f"âœ… æˆåŠŸå¤„ç†çš„é…ç½® (ä¸»æµç¨‹): {success_datasets}")
    print(f"âŒ å¤±è´¥çš„é…ç½® (ä¸»æµç¨‹): {failed_datasets}")

# =================================================================
# <<< æ–°å¢ä»£ç ï¼šåœ¨è„šæœ¬ç»“æŸæ—¶æŠ¥å‘Šæ€»ä½“æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ >>>
# =================================================================
finally:
    script_end_time = time.time()
    total_duration_seconds = script_end_time - script_start_time

    print("\n\n" + "=" * 20 + " æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š " + "=" * 20)
    print(f"ğŸ“Š è„šæœ¬æ€»è¿è¡Œæ—¶é—´: {total_duration_seconds:.2f} ç§’ ({total_duration_seconds / 60:.2f} åˆ†é’Ÿ)")

    if psutil_available:
        # è·å–è„šæœ¬ç»“æŸæ—¶çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        end_memory_rss = process.memory_info().rss
        net_memory_increase = end_memory_rss - start_memory_rss

        print("\n--- å†…å­˜ä½¿ç”¨æƒ…å†µè¯´æ˜ ---")
        print("å†…å­˜ä½¿ç”¨æ˜¯é€šè¿‡ 'psutil' åº“è®¡ç®—çš„ã€‚æˆ‘ä»¬ä¸»è¦å…³æ³¨ RSS (Resident Set Size)ã€‚")
        print("RSS æŒ‡çš„æ˜¯è¿›ç¨‹åœ¨ç‰©ç†å†…å­˜(RAM)ä¸­å ç”¨çš„éƒ¨åˆ†ï¼Œæ˜¯è¡¡é‡å®é™…å†…å­˜æ¶ˆè€—çš„å¸¸ç”¨æŒ‡æ ‡ã€‚")
        print("ç”±äºPythonçš„åŠ¨æ€å†…å­˜ç®¡ç†ï¼Œç²¾ç¡®çš„â€œå³°å€¼â€å†…å­˜éš¾ä»¥åœ¨ä¸ä¾µå…¥ä»£ç çš„æƒ…å†µä¸‹æ•è·ã€‚")
        print("æˆ‘ä»¬åœ¨æ­¤æŠ¥å‘Šè„šæœ¬è¿è¡Œç»“æŸæ—¶çš„æœ€ç»ˆå†…å­˜å ç”¨ï¼Œå®ƒèƒ½å¾ˆå¥½åœ°åæ˜ å¤§è§„æ¨¡æ•°æ®å¤„ç†åçš„å†…å­˜çŠ¶æ€ã€‚")
        print("--------------------------")
        print(f"ğŸ§  åˆå§‹å†…å­˜å ç”¨ (RSS): {start_memory_rss / 1024 ** 2:.2f} MB")
        print(f"ğŸ§  æœ€ç»ˆå†…å­˜å ç”¨ (RSS): {end_memory_rss / 1024 ** 2:.2f} MB")
        print(f"ğŸ§  å‡€å¢å†…å­˜å ç”¨ (RSS): {net_memory_increase / 1024 ** 2:.2f} MB")

        # å°†å­—èŠ‚è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„å•ä½
        if end_memory_rss > 1024 ** 3:
            print(f"   (æœ€ç»ˆå†…å­˜å ç”¨çº¦ä¸º: {end_memory_rss / 1024 ** 3:.2f} GB)")
        else:
            print(f"   (æœ€ç»ˆå†…å­˜å ç”¨çº¦ä¸º: {end_memory_rss / 1024 ** 2:.2f} MB)")
    else:
        print("\nğŸ§  å†…å­˜ä½¿ç”¨æƒ…å†µï¼šæœªç»Ÿè®¡ (éœ€è¦å®‰è£… 'psutil' åº“)ã€‚")
    print("=" * 55)
