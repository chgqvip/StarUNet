import os
import tensorflow as tf
import numpy as np
import random
import traceback
import gc
import sys
from tqdm import tqdm
import SarOptMatch  # ä¸»åŒ…
from SarOptMatch.train_modules import BestModelSaver, evaluate_best_model, plot_training_curve
from SarOptMatch.log import setup_logger  # å¯¼å…¥æ­£ç¡®çš„ setup_logger
from tensorflow.keras import backend as K
from SarOptMatch.loss_module import make_loss, LossComponentsLogger
import SarOptMatch.dataset as dataset_module
import subprocess
import tracemalloc
# <<< MODIFIED END >>>
import glob
# ... (å…¶ä½™ä»£ç )

# === âœ… æ˜¯å¦è®­ç»ƒæ¨¡å‹ï¼ŒFalse åˆ™åªåŠ è½½æƒé‡åšæ¨ç† ===
train = False

script_dir = os.path.dirname(os.path.abspath(__file__))

# === ç½‘æ ¼æœç´¢å‚æ•°å®šä¹‰ (ä½¿ç”¨ alpha_prime) ===
ALPHA_PRIME_LIST = [0,0.25]
THRESHOLD_LIST = [2]
# ALPHA_PRIME_LIST = [0,0.25,0.5,0.75,1]
# THRESHOLD_LIST = [2,4,6]
# === GPU è®¾ç½® ===
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# === å›ºå®šéšæœºç§å­ ===
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# === æ•°æ®é›†è·¯å¾„ ===
# dataset_paths = ['E:\sen1-2dataset',r'E:\OSdataset\OSdataset-5\OSdataset\256','E:\AOI_11_Rotterdam\TEST',]
# ims_per_folder_list = [100,9623,11164]
dataset_paths = ['E:\sen1-2dataset',]
ims_per_folder_list = [100]
# dataset_paths = [r'E:\OSdataset\OSdataset-5\OSdataset\256']
# ims_per_folder_list = [9623]
# dataset_paths = ['E:\AOI_11_Rotterdam\TEST',]
# ims_per_folder_list = [11164]
# Monkey patch dataset ä¸­çš„ blur
def monkey_patch_threshold(thresh):
    original_blur = dataset_module.gaussian_blur

    def patched_blur(threshold_tensor, offsets):
        if not isinstance(thresh, tf.Tensor):
            threshold_to_use = tf.constant(float(thresh), dtype=tf.float32)
        else:
            threshold_to_use = tf.cast(thresh, dtype=tf.float32)
        return original_blur(threshold_to_use, offsets)

    dataset_module.gaussian_blur = patched_blur


# === æ¸…ç©ºæ—§çš„ grid_results.csv æ–‡ä»¶ ===
csv_path = os.path.join(script_dir, "grid_results.csv")  # script_dir è€Œä¸æ˜¯ os.path.dirname(__file__)
# åœ¨ main.py çš„é¡¶éƒ¨ï¼ŒCSV åˆå§‹åŒ–éƒ¨åˆ†ï¼š
if train:
    print(f"ğŸ§¹ åˆå§‹åŒ–/æ¸…ç©º grid_results.csv æ–‡ä»¶...")
    with open(csv_path, "w") as f:
        pass # åªæ¸…ç©ºæ–‡ä»¶ï¼Œä¸å†™å…¥ä»»ä½•å†…å®¹


# === æ ¡å‡†å‡½æ•° (è·å– S_ce å’Œ S_mse) ===
def calibrate_scaling_factors(dataset_path_for_cal, ims_per_folder_for_cal, seed_val, base_config, cal_threshold_val):
    print("\n--- ğŸš€ å¼€å§‹æ ¡å‡† S_ce å’Œ S_mse ---")
    s_ce_val = 3.0
    s_mse_val = 0.5

    monkey_patch_threshold(cal_threshold_val)
    print(f"æ ¡å‡†æ—¶ä½¿ç”¨çš„ Threshold: {cal_threshold_val}")

    try:
        cal_ims_count = max(20, ims_per_folder_for_cal // 10)
        print(f"æ ¡å‡†ä½¿ç”¨å›¾åƒæ•°: {cal_ims_count} from {dataset_path_for_cal}")
        sar_files_cal, opt_files_cal, offsets_cal = SarOptMatch.dataset.sen1_2(
            data_path=dataset_path_for_cal, seed=seed_val, ims_per_folder=cal_ims_count
        )
        # â¬‡ï¸ **ä¿®æ”¹ç‚¹ 1: ç§»é™¤ split_data è°ƒç”¨ä¸­çš„ val_split å‚æ•°**
        _, val_data_cal, _ = SarOptMatch.dataset.split_data(  # _ è¡¨ç¤ºä¸ä½¿ç”¨ training_data_cal
            sar_files_cal, opt_files_cal, offsets_cal,
            batch_size=4, seed=seed_val, masking_strategy="unet"  # ç§»é™¤äº† val_split
        )
        if val_data_cal is None or tf.data.experimental.cardinality(val_data_cal).numpy() == 0:
            print("âŒ æ ¡å‡†æ—¶éªŒè¯æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­æ ¡å‡†ã€‚")
            return s_ce_val, s_mse_val
    except Exception as e:
        print(f"âŒ æ ¡å‡†æ—¶æ•°æ®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return s_ce_val, s_mse_val

    # --- æ ¡å‡† S_ce ---
    print("\n--- æ ¡å‡† S_ce (alpha_prime = 0.0) ---")
    try:
        K.clear_session()
        gc.collect()
        matcher_cal_ce = SarOptMatch.architectures.SAR_opt_Matcher()
        matcher_cal_ce.create_model(**base_config)
        matcher_cal_ce.model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),
                                     loss=make_loss(alpha_prime=0.0, S_ce=1.0, S_mse=1.0))
        temp_logger_ce = LossComponentsLogger(validation_data=val_data_cal, S_ce=1.0, S_mse=1.0)
        matcher_cal_ce.model.fit(val_data_cal.take(10), epochs=3, verbose=0, callbacks=[temp_logger_ce])
        if temp_logger_ce.ce_raw_history:
            s_ce_val = np.mean(temp_logger_ce.ce_raw_history[-2:])
            print(f"âœ… æ ¡å‡†å¾—åˆ° S_ce ä¼°è®¡å€¼: {s_ce_val:.4f}")
        else:
            print("âš ï¸ æ ¡å‡† S_ce æ—¶æœªè·å–åˆ° ce_raw_historyã€‚")
        del matcher_cal_ce, temp_logger_ce
    except Exception as e:
        print(f"âŒ æ ¡å‡† S_ce å¤±è´¥: {e}")
        traceback.print_exc()

    # --- æ ¡å‡† S_mse ---
    print("\n--- æ ¡å‡† S_mse (alpha_prime = 1.0) ---")
    try:
        K.clear_session()
        gc.collect()
        matcher_cal_mse = SarOptMatch.architectures.SAR_opt_Matcher()
        matcher_cal_mse.create_model(**base_config)
        matcher_cal_mse.model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),
                                      loss=make_loss(alpha_prime=1.0, S_ce=1.0, S_mse=1.0))
        temp_logger_mse = LossComponentsLogger(validation_data=val_data_cal, S_ce=1.0, S_mse=1.0)
        matcher_cal_mse.model.fit(val_data_cal.take(10), epochs=3, verbose=0, callbacks=[temp_logger_mse])
        if temp_logger_mse.mse_sum_raw_history:
            s_mse_val = np.mean(temp_logger_mse.mse_sum_raw_history[-2:])
            print(f"âœ… æ ¡å‡†å¾—åˆ° S_mse ä¼°è®¡å€¼: {s_mse_val:.4f}")
        else:
            print("âš ï¸ æ ¡å‡† S_mse æ—¶æœªè·å–åˆ° mse_sum_raw_historyã€‚")
        del matcher_cal_mse, temp_logger_mse
    except Exception as e:
        print(f"âŒ æ ¡å‡† S_mse å¤±è´¥: {e}")
        traceback.print_exc()

    final_s_ce = max(float(s_ce_val), 1e-5)
    final_s_mse = max(float(s_mse_val), 1e-5)
    print(f"--- âœ… æ ¡å‡†å®Œæˆ: S_ce = {final_s_ce:.4f}, S_mse = {final_s_mse:.4f} ---")
    K.clear_session()
    gc.collect()
    return final_s_ce, final_s_mse


# === åœ¨ä¸»å¾ªç¯å¼€å§‹å‰æ‰§è¡Œæ ¡å‡† ===
S_CE_CALIBRATED = None
S_MSE_CALIBRATED = None
MODEL_CONFIG = {
    'reference_im_shape': (256, 256, 1), 'floating_im_shape': (192, 192, 1),
    'normalize': True, 'backbone': 'UnetPlus', 'n_filters': 32,
    'star_mlp_ratio': 4, 'star_drop_path_rate': 0.0,
    'use_conv_downsampling': True, 'decoder_upsampling_method': 'transpose_conv',
    'multiscale': False, 'attention': False, 'activation': 'relu'
}
CALIBRATION_THRESHOLD = THRESHOLD_LIST[0] if THRESHOLD_LIST else 2.0

if train and dataset_paths:
    print(f"ä½¿ç”¨æ•°æ®é›† '{dataset_paths[0]}' å’Œ threshold={CALIBRATION_THRESHOLD} è¿›è¡Œæ ¡å‡†...")
    S_CE_CALIBRATED, S_MSE_CALIBRATED = calibrate_scaling_factors(
        dataset_paths[0], ims_per_folder_list[0], seed, MODEL_CONFIG, CALIBRATION_THRESHOLD
    )
    if S_CE_CALIBRATED is None or S_MSE_CALIBRATED is None or S_CE_CALIBRATED < 1e-5 or S_MSE_CALIBRATED < 1e-5:
        print("âŒ æ ¡å‡†å¤±è´¥æˆ–å€¼è¿‡å°ï¼Œå°†ä½¿ç”¨é»˜è®¤ S_ce=1.0, S_mse=1.0ã€‚ç»“æœå¯èƒ½ä¸ç†æƒ³ã€‚")
        S_CE_CALIBRATED = 1.0
        S_MSE_CALIBRATED = 1.0

# === ä¸»å¾ªç¯ ===
for dataset_path, ims_per_folder in tqdm(zip(dataset_paths, ims_per_folder_list),
                                         total=len(dataset_paths),
                                         desc="ğŸ› ï¸ æ•°æ®é›†å¤„ç†è¿›åº¦",
                                         file=sys.__stdout__,  # tqdm è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
                                         leave=False):
    success_datasets = []
    failed_datasets = []
    base_name = os.path.basename(dataset_path)

    for alpha_prime in ALPHA_PRIME_LIST:
        for threshold in THRESHOLD_LIST:
            current_run_description = f"alpha_prime={alpha_prime:.2f}, threshold={threshold}"
            print(f"\nğŸ”¬ å½“å‰å‚æ•°ç»„åˆ: {current_run_description}")  # è¿™ä¸ªprintä¼šè¢«loggeræ•è·

            monkey_patch_threshold(threshold)

            ap_str = f"{alpha_prime:.2f}".replace('.', 'p')
            param_suffix = f"ap{ap_str}_t{threshold}"
            dataset_name_for_log = f"{base_name}_{param_suffix}"  # ç”¨äºæ—¥å¿—å’Œæ¨¡å‹åç§°

            # â¬‡ï¸ **ä¿®æ”¹ç‚¹ 2: setup_logger è°ƒç”¨**
            # StreamLogger ä¼šæ¥ç®¡ sys.stdout, æ‰€ä»¥ setup_logger åº”è¯¥åœ¨å…¶ä»– print ä¹‹å‰è¢«è°ƒç”¨ï¼Œ
            # æˆ–è€…ç¡®ä¿å®ƒçš„ print è¯­å¥æ˜¯æˆ‘ä»¬æœŸæœ›çš„æ—¥å¿—åˆå§‹åŒ–ä¿¡æ¯ã€‚
            # setup_logger çš„ logs_dir é»˜è®¤ä¸º "logs"ã€‚å¦‚æœæƒ³ç”¨ "SarOptMatch_logs"ï¼Œåˆ™æŒ‡å®šã€‚
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ setup_logger çš„é»˜è®¤ logs_dir="logs"
            logger_instance = setup_logger(dataset_name_for_log, logs_dir=os.path.join(script_dir, "SarOptMatch_logs"))
            # logger_instance ç°åœ¨æ˜¯ StreamLogger çš„å®ä¾‹

            # åç»­çš„ print ä¼šé€šè¿‡ StreamLogger.write è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
            logger_instance.write(
                f"INFO: å¼€å§‹å¤„ç†: {dataset_name_for_log} | æ¯ä¸ªæ–‡ä»¶å¤¹å›¾åƒæ•°: {ims_per_folder}\n")  # ä½¿ç”¨ logger.write
            logger_instance.write(f"INFO: å‚æ•°: alpha_prime={alpha_prime}, threshold={threshold}\n")
            if train:
                logger_instance.write(
                    f"INFO: æ ¡å‡†å¾—åˆ°çš„ç¼©æ”¾å› å­: S_ce={S_CE_CALIBRATED:.4f}, S_mse={S_MSE_CALIBRATED:.4f}\n")

            try:
                K.clear_session()
                gc.collect()

                logger_instance.write("INFO: åŠ è½½æ•°æ®...\n")
                sar_files, opt_files, offsets = SarOptMatch.dataset.sen1_2(
                    data_path=dataset_path, seed=seed, ims_per_folder=ims_per_folder
                )
                training_data, validation_data, validation_dataRGB = SarOptMatch.dataset.split_data(
                    sar_files, opt_files, offsets, batch_size=4, seed=seed, masking_strategy="unet"
                )
                logger_instance.write("INFO: æ•°æ®åŠ è½½å®Œæˆã€‚\n")

                matcher = SarOptMatch.architectures.SAR_opt_Matcher()
                matcher.create_model(**MODEL_CONFIG)
                matcher.model_name = dataset_name_for_log  # ä½¿ç”¨ä¸æ—¥å¿—ä¸€è‡´çš„åç§°
                logger_instance.write(f"INFO: æ¨¡å‹ {matcher.model_name} åˆ›å»ºå®Œæˆã€‚\n")

                if train:
                    initial_learning_rate = 0.0005
                    optimizer_for_training = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)

                    current_s_ce_to_use = S_CE_CALIBRATED if S_CE_CALIBRATED is not None else 1.0
                    current_s_mse_to_use = S_MSE_CALIBRATED if S_MSE_CALIBRATED is not None else 1.0

                    loss_function_instance = make_loss(
                        alpha_prime=float(alpha_prime),
                        S_ce=float(current_s_ce_to_use),
                        S_mse=float(current_s_mse_to_use)
                    )
                    matcher.model.compile(optimizer=optimizer_for_training, loss=loss_function_instance)
                    logger_instance.write(
                        f"INFO: æ¨¡å‹ç¼–è¯‘å®Œæˆï¼Œä½¿ç”¨ alpha_prime={alpha_prime}, S_ce={current_s_ce_to_use:.4f}, S_mse={current_s_mse_to_use:.4f}\n")

                    loss_logger_callback = LossComponentsLogger(
                        validation_data=validation_data,
                        S_ce=float(current_s_ce_to_use),
                        S_mse=float(current_s_mse_to_use),
                        log_prefix='val_'
                    )
                    logger_instance.write("INFO: LossComponentsLogger å®ä¾‹åŒ–å®Œæˆã€‚\n")

                    # å°† logger_instance (StreamLogger) ä¼ é€’ç»™ BestModelSaver (å¦‚æœå®ƒæ”¯æŒ)
                    # å‡è®¾ BestModelSaver å†…éƒ¨ä¼šä½¿ç”¨ print()ï¼Œè¿™äº› print ä¼šè¢« StreamLogger æ•è·
                    best_model_saver = BestModelSaver(validation_data, matcher,
                                                      dataset_name_for_log)  # ç§»é™¤ logger_instance
                    all_callbacks = [best_model_saver, loss_logger_callback]

                    logger_instance.write("INFO: å¼€å§‹è®­ç»ƒ...\n")
                    # Keras çš„ fit æ–¹æ³•çš„ verbose è¾“å‡º (è¿›åº¦æ¡ç­‰) ä¼šè¢« StreamLogger è¿‡æ»¤
                    history, early_stopped = matcher.train(
                        training_data, validation_data, epochs=10, callbacks=all_callbacks
                    )
                    logger_instance.write(f"INFO: è®­ç»ƒå®Œæˆã€‚Early stopped: {early_stopped}\n")

                    # plot_training_curve å†…éƒ¨çš„ print ä¹Ÿä¼šè¢«æ•è·
                    plot_training_curve(dataset_name_for_log)  # ç§»é™¤ logger_instance

                    with open(csv_path, "a") as f:
                        f.write(
                            f"{dataset_name_for_log},{alpha_prime:.2f},{threshold},{best_model_saver.best_score:.4f}\n")
                    logger_instance.write(f"INFO: ç»“æœå·²å†™å…¥ {csv_path}\n")

                    logger_instance.write("INFO: åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°...\n")
                    evaluate_best_model(matcher, validation_data, validation_dataRGB
                                       )  # ç§»é™¤ logger_instance
                    success_datasets.append(dataset_name_for_log)
                    logger_instance.write("INFO: è¯„ä¼°å®Œæˆã€‚\n")

                else:  # æ¨ç†æ¨¡å¼
                    weight_name = f"{dataset_name_for_log}_best_model.h5"
                    weight_path = os.path.join("lambda_models", weight_name)
                    logger_instance.write(f"INFO: æ¨ç†æ¨¡å¼ï¼Œå°è¯•åŠ è½½æƒé‡: {weight_path}\n")
                    if not os.path.isfile(weight_path):
                        logger_instance.write(f"ERROR: æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡æ–‡ä»¶: {weight_path}\n")  # ä½¿ç”¨ logger.write
                        failed_datasets.append(dataset_name_for_log)
                    else:
                        matcher.model.load_weights(weight_path)
                        logger_instance.write(f"INFO: æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {weight_path}\n")
                        evaluate_best_model(matcher, validation_data, validation_dataRGB,
                                            dataset_name_for_log)
                        success_datasets.append(dataset_name_for_log)
                        logger_instance.write("INFO: æ¨ç†è¯„ä¼°å®Œæˆã€‚\n")

            except Exception as e:
                # ä½¿ç”¨ logger_instance.write è®°å½•å¼‚å¸¸
                error_msg = traceback.format_exc()
                logger_instance.write(f"ERROR: æ•°æ®é›† {dataset_name_for_log} å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯ï¼š\n{error_msg}\n")
                failed_datasets.append(dataset_name_for_log)
            finally:
                if 'logger_instance' in locals() and hasattr(logger_instance, 'close'):
                    logger_instance.write(f"INFO: å®Œæˆå¤„ç†: {dataset_name_for_log}\n\n")
                    logger_instance.close()  # å…³é—­å½“å‰ StreamLoggerï¼Œæ¢å¤ stdout
                K.clear_session()
                gc.collect()

    # === æ€»ç»“è¾“å‡º ===
    # ä¸ºæ€»ç»“åˆ›å»ºä¸€ä¸ªæ–°çš„ã€ä¸´æ—¶çš„ StreamLogger æˆ–ä½¿ç”¨æ ‡å‡† print
    # å¦‚æœåœ¨è¿™é‡Œå†æ¬¡è°ƒç”¨ setup_loggerï¼Œå®ƒä¼šå†æ¬¡æ¥ç®¡ stdout
    # ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œçš„æ€»ç»“ç›´æ¥ç”¨ printï¼Œå®ƒä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä½†ä¸ä¼šè¢«ç‰¹å®šçš„æ–‡ä»¶æ—¥å¿—è®°å½•ï¼ˆé™¤éæœ‰å…¨å±€çš„ stdout æ•è·ï¼‰
    # æˆ–è€…ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªä¸“é—¨çš„æ€»ç»“æ—¥å¿—æ–‡ä»¶
    summary_log_path = os.path.join(script_dir, "SarOptMatch_logs", f"{base_name}_summary.log")
    with open(summary_log_path, "a", encoding="utf-8") as summary_f:
        summary_f.write(f"\n=== æ•°æ®é›† {base_name} å¤„ç†æ€»ç»“ ===\n")
        summary_f.write(f"âœ… æˆåŠŸå¤„ç†çš„å‚æ•°ç»„åˆ: {success_datasets}\n")
        summary_f.write(f"âŒ å¤±è´¥çš„å‚æ•°ç»„åˆ: {failed_datasets}\n")
    print(f"\n=== æ•°æ®é›† {base_name} å¤„ç†æ€»ç»“ ===")  # ä¹Ÿä¼šè¾“å‡ºåˆ°æ§åˆ¶å°
    print(f"âœ… æˆåŠŸå¤„ç†çš„å‚æ•°ç»„åˆ: {success_datasets}")
    print(f"âŒ å¤±è´¥çš„å‚æ•°ç»„åˆ: {failed_datasets}")

    # === è°ƒç”¨ plot_grid_results.py ç»˜å›¾ ===
    plot_script_path = os.path.join(script_dir, "SarOptMatch", "plot_grid_results.py")
    if not os.path.isfile(plot_script_path):
        print(f"âŒ æ‰¾ä¸åˆ°ç»˜å›¾è„šæœ¬ plot_grid_results.pyï¼ŒæœŸæœ›è·¯å¾„: {plot_script_path}")
    else:
        try:
            print(f"ğŸ“Š æ•°æ®é›† {base_name} æ‰€æœ‰å‚æ•°ç»„åˆè®­ç»ƒå®Œæˆï¼Œç”Ÿæˆçƒ­åŠ›å›¾...")
            subprocess.run(["python", plot_script_path, "--csv_file", csv_path], check=True)
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")

print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæ¯•ã€‚")
